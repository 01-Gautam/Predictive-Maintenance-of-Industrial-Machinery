{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"content\"></a>\n",
    "## Notebook content\n",
    "\n",
    "This notebook contains code to resume and continue training an AutoAI pipeline partially trained in an AutoAI experiment. If there is additional training data, the notebook retrieves the data in batches and incrementally trains the model, then tests the model.\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses python 3.11 and scikit-learn 1.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Notebook goals\n",
    "\n",
    "This notebook introduces new commands and demonstrates techniques to support incremental learning, including: \n",
    "\n",
    "-  Data reader (read data in batches)\n",
    "-  Incremental learning (`partial_fit`)\n",
    "-  Pipeline evaluation\n",
    "\n",
    "## Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "**[Setup](#Setup)**<br>\n",
    "&nbsp;&nbsp;[Package installation](#Package-installation)<br>\n",
    "&nbsp;&nbsp;[AutoAI experiment metadata](#AutoAI-experiment-metadata)<br>\n",
    "&nbsp;&nbsp;[watsonx.ai connection](#watsonx.ai-connection)<br>\n",
    "**[Incremental learning](#Incremental-learning)** <br>\n",
    "&nbsp;&nbsp;[Get pipeline](#Get-pipeline)<br>\n",
    "&nbsp;&nbsp;[Read training data (DataLoader)](#Data-loader)<br>\n",
    "&nbsp;&nbsp;[Incrementally train pipeline model](#Continue-model-training)<br>\n",
    "&nbsp;&nbsp;[Test pipeline model](#Test-pipeline-model)<br>\n",
    "**[Store the model](#Store-the-model)**<br>\n",
    "**[Create online deployment](#Create-online-deployment)**<br>\n",
    "&nbsp;&nbsp;[Working with spaces](#Working-with-spaces)<br>\n",
    "**[Summary and next steps](#Summary-and-next-steps)**<br>\n",
    "**[Copyrights](#Copyrights)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"install\"></a>\n",
    "## Package installation\n",
    "Before you use the sample code in this notebook, install the following packages:\n",
    " - ibm-watsonx-ai,\n",
    " - autoai-libs,\n",
    " - scikit-learn,\n",
    " - snapml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T14:00:45.009458Z",
     "iopub.status.busy": "2020-10-12T14:00:45.007968Z",
     "iopub.status.idle": "2020-10-12T14:00:46.037702Z",
     "shell.execute_reply": "2020-10-12T14:00:46.038270Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install ibm-watsonx-ai | tail -n 1\n",
    "!pip install autoai-libs~=2.0 | tail -n 1\n",
    "!pip install scikit-learn==1.3.* | tail -n 1\n",
    "!pip install -U lale~=0.8.3 | tail -n 1\n",
    "!pip install snapml==1.14.* | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"variables_definition\"></a>\n",
    "## AutoAI experiment metadata\n",
    "The following cell contains the training data connection details.  \n",
    "**Note**: The connection might contain authorization credentials, so be careful when sharing the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T14:00:49.797633Z",
     "iopub.status.busy": "2020-10-12T14:00:49.796778Z",
     "iopub.status.idle": "2020-10-12T14:00:57.182715Z",
     "shell.execute_reply": "2020-10-12T14:00:57.183132Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.helpers import DataConnection\n",
    "from ibm_watsonx_ai.helpers import ContainerLocation\n",
    "\n",
    "training_data_references = [\n",
    "    DataConnection(\n",
    "        data_asset_id='cd6216e8-a154-43f5-9321-d460b67c8cc0'\n",
    "    ),\n",
    "]\n",
    "training_result_reference = DataConnection(\n",
    "    location=ContainerLocation(\n",
    "        path='auto_ml/6ff8daf0-9da5-4921-ab3d-a74f209ddab3/wml_data/cd2fda5e-dd7e-4de2-8cb5-4b350c1a9166/data/automl',\n",
    "        model_location='auto_ml/6ff8daf0-9da5-4921-ab3d-a74f209ddab3/wml_data/cd2fda5e-dd7e-4de2-8cb5-4b350c1a9166/data/automl/model.zip',\n",
    "        training_status='auto_ml/6ff8daf0-9da5-4921-ab3d-a74f209ddab3/wml_data/cd2fda5e-dd7e-4de2-8cb5-4b350c1a9166/training-status.json'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains input parameters provided to run the AutoAI experiment in Watson Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-12T14:00:57.187305Z",
     "iopub.status.busy": "2020-10-12T14:00:57.186602Z",
     "iopub.status.idle": "2020-10-12T14:00:57.188392Z",
     "shell.execute_reply": "2020-10-12T14:00:57.188878Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "experiment_metadata = dict(\n",
    "    prediction_type='multiclass',\n",
    "    prediction_column='Failure Type',\n",
    "    holdout_size=0.1,\n",
    "    scoring='accuracy',\n",
    "    csv_separator=',',\n",
    "    random_state=33,\n",
    "    max_number_of_estimators=2,\n",
    "    training_data_references=training_data_references,\n",
    "    training_result_reference=training_result_reference,\n",
    "    deployment_url='https://au-syd.ml.cloud.ibm.com',\n",
    "    project_id='b4876675-29f5-4f81-99ad-7661e2560827',\n",
    "    train_sample_columns_index_list=[2, 3, 4, 5, 6, 7, 8],\n",
    "    drop_duplicates=True,\n",
    "    include_batched_ensemble_estimators=['BatchedTreeEnsembleClassifier(ExtraTreesClassifier)', 'BatchedTreeEnsembleClassifier(LGBMClassifier)', 'BatchedTreeEnsembleClassifier(RandomForestClassifier)', 'BatchedTreeEnsembleClassifier(SnapBoostingMachineClassifier)', 'BatchedTreeEnsembleClassifier(SnapRandomForestClassifier)', 'BatchedTreeEnsembleClassifier(XGBClassifier)'],\n",
    "    classes=['Heat Dissipation Failure', 'No Failure', 'Overstrain Failure', 'Power Failure', 'Random Failures', 'Tool Wear Failure'],\n",
    "    feature_selector_mode='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set `n_jobs` parameter to the number of available CPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, ast\n",
    "CPU_NUMBER = 4\n",
    "if 'RUNTIME_HARDWARE_SPEC' in os.environ:\n",
    "    CPU_NUMBER = int(ast.literal_eval(os.environ['RUNTIME_HARDWARE_SPEC'])['num_cpu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"connection\"></a>\n",
    "## watsonx.ai connection\n",
    "\n",
    "This cell defines the credentials required to work with the watsonx.ai Runtime.\n",
    "\n",
    "**Action**: Provide the IBM Cloud apikey, For details, see [documentation](https://cloud.ibm.com/docs/account?topic=account-userapikey)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "api_key = getpass.getpass(\"Please enter your api key (press enter): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai import Credentials\n",
    "\n",
    "credentials = Credentials(\n",
    "    api_key=api_key,\n",
    "    url=experiment_metadata['deployment_url']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai import APIClient\n",
    "\n",
    "client = APIClient(credentials)\n",
    "\n",
    "if 'space_id' in experiment_metadata:\n",
    "    client.set.default_space(experiment_metadata['space_id'])\n",
    "else:\n",
    "    client.set.default_project(experiment_metadata['project_id'])\n",
    "\n",
    "training_data_references[0].set_client(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"incremental_learning\"></a>\n",
    "# Incremental learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"preview_model_to_python_code\"></a>\n",
    "## Get pipeline\n",
    "\n",
    "Download and save a pipeline model object from the AutoAI training job (`lale` pipeline type is used for inspection and `partial_fit` capabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.experiment import AutoAI\n",
    "\n",
    "pipeline_optimizer = AutoAI(credentials, project_id=experiment_metadata['project_id']).runs.get_optimizer(metadata=experiment_metadata)\n",
    "pipeline_model = pipeline_optimizer.get_pipeline(pipeline_name='Pipeline_5', astype='lale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_loader\"></a>\n",
    "## Data loader\n",
    "\n",
    "Create DataLoader iterator to retrieve training dataset in batches. DataLoader is `Torch` compatible (`torch.utils.data`), returning Pandas DataFrames.\n",
    "\n",
    "**Note**: If reading data results in an error, provide data as iterable reader (e.g. `read_csv()` method from Pandas with chunks). It may be necessary to use methods for initial data pre-processing like: e.g. `DataFrame.dropna()`, `DataFrame.drop_duplicates()`, `DataFrame.sample()`.\n",
    "\n",
    "```\n",
    "reader_full_data = pd.read_csv(DATA_PATH, chunksize=CHUNK_SIZE)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch size in rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_batch_rows = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.data_loaders import experiment as data_loaders\n",
    "from ibm_watsonx_ai.data_loaders.datasets import experiment as datasets\n",
    "\n",
    "dataset = datasets.ExperimentIterableDataset(\n",
    "    connection=training_data_references[0],\n",
    "    enable_sampling=False,\n",
    "    experiment_metadata=experiment_metadata,\n",
    "    number_of_batch_rows=number_of_batch_rows\n",
    "    )\n",
    "    \n",
    "data_loader = data_loaders.ExperimentDataLoader(dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "## Continue model training\n",
    "\n",
    "In this cell, the pipeline is incrementally fitted using data batches (via `partial_fit` calls).\n",
    "\n",
    "**Note**: If you need, you can evaluate the pipeline using custom holdout data. Provide the `X_test`, `y_test` and call `scorer` on them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define scorer from the optimization metric\n",
    "This cell constructs the cell scorer based on the experiment metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import get_scorer\n",
    "\n",
    "scorer = get_scorer(experiment_metadata['scoring'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the incremental learner\n",
    "\n",
    "For the best training performance set:\n",
    "\n",
    "- `n_jobs` - to available number of CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model.steps[-1][1].impl.base_ensemble.set_params(n_jobs=CPU_NUMBER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a learning curve plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ibm_watsonx_ai.utils.autoai.incremental import plot_learning_curve\n",
    "import time\n",
    "\n",
    "partial_fit_scores = []\n",
    "fit_times = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"test_model\"></a>\n",
    "### Fit pipeline model in batches\n",
    "\n",
    "**Tip**: If the data passed to `partial_fit` is highly imbalanced (>1:10), please consider applying the `sample_weight` parameter:\n",
    "\n",
    "```\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "pipeline_model.partial_fit(X_train, y_train, freeze_trained_prefix=True,\n",
    "                                             sample_weight=compute_sample_weight('balanced', y_train))\n",
    "```                                                                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If you have a holdout/test set please provide it for better pipeline evaluation and replace X_test and y_test in the following cell.\n",
    "\n",
    "```\n",
    "from pandas import read_csv\n",
    "test_df = read_csv('DATA_PATH')\n",
    "\n",
    "X_test = test_df.drop([experiment_metadata['prediction_column']], axis=1).values\n",
    "y_test = test_df[experiment_metadata['prediction_column']].values\n",
    "```\n",
    "\n",
    "If holdout set was not provided, 30% of first training batch would be used as holdout.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter warnings for incremental training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "for i, batch_df in enumerate(data_loader):\n",
    "    batch_df.dropna(subset=experiment_metadata[\"prediction_column\"], inplace=True)\n",
    "    X_train = batch_df.drop([experiment_metadata['prediction_column']], axis=1).values\n",
    "    y_train = batch_df[experiment_metadata['prediction_column']].values\n",
    "    if i==0:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3)\n",
    "    start_time = time.time()\n",
    "    pipeline_model = pipeline_model.partial_fit(X_train, y_train, freeze_trained_prefix=True)\n",
    "    fit_times.append(time.time() - start_time)\n",
    "    partial_fit_scores.append(scorer(pipeline_model, X_test, y_test))\n",
    "    plot_learning_curve(fig=fig, axes=axes, scores=partial_fit_scores, fit_times=fit_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"test_model\"></a>\n",
    "## Test pipeline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Test the fitted pipeline (`predict`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_model.predict(X_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"saving\"></a>\n",
    "## Store the model\n",
    "\n",
    "In this section you will learn how to store the incrementally trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metadata = {\n",
    "    client.repository.ModelMetaNames.NAME: 'P5 - Pretrained AutoAI pipeline'\n",
    "}\n",
    "\n",
    "stored_model_details = client.repository.store_model(model=pipeline_model, meta_props=model_metadata, experiment_metadata=experiment_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the stored model details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_model_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"deployment\"></a>\n",
    "## Create online deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the commands below to promote the model to space and create online deployment (web service).\n",
    "\n",
    "<a id=\"working_spaces\"></a>\n",
    "### Working with spaces\n",
    "\n",
    "In this section you will specify a deployment space for organizing the assets for deploying and scoring the model. If you do not have an existing space, you can use <a href=\"https://au-syd.dai.cloud.ibm.com/ml-runtime/dashboard?context=wx\">Deployment Spaces Dashboard</a> to create a new space, following these steps:\n",
    "\n",
    "- Click **New Deployment Space**.\n",
    "- Create an empty space.\n",
    "- Select Cloud Object Storage.\n",
    "- Select watsonx.ai Runtime and press **Create**.\n",
    "- Copy `space_id` and paste it below.\n",
    "\n",
    "**Tip**: You can also use the API to prepare the space for your work. Learn more [here](https://github.com/IBM/watson-machine-learning-samples/blob/master/cloud/notebooks/python_sdk/instance-management/Space%20management.ipynb).\n",
    "\n",
    "**Info**: Below cells are `raw` type - in order to run them, change their type to `code` and run them (no need to restart the notebook). You may need to add some additional info (see the **action** below).\n",
    "\n",
    "**Action**: Assign or update space ID below.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "space_id = \"PUT_YOUR_SPACE_ID_HERE\"\n",
    "\n",
    "model_id = client.spaces.promote(asset_id=stored_model_details[\"metadata\"][\"id\"], source_project_id=experiment_metadata[\"project_id\"], target_space_id=space_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare online deployment"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "client.set.default_space(space_id)\n",
    "\n",
    "deploy_meta = {\n",
    "        client.deployments.ConfigurationMetaNames.NAME: \"Incrementally trained AutoAI pipeline\",\n",
    "        client.deployments.ConfigurationMetaNames.ONLINE: {},\n",
    "    }\n",
    "\n",
    "deployment_details = client.deployments.create(artifact_uid=model_id, meta_props=deploy_meta)\n",
    "deployment_id = client.deployments.get_id(deployment_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test online deployment"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scoring_payload = {\n",
    "    \"input_data\": [{\n",
    "        'values': X_test[:5]\n",
    "    }]\n",
    "}\n",
    "\n",
    "client.deployments.score(deployment_id, scoring_payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cleanup\"></a>\n",
    "### Deleting deployment\n",
    "You can delete the existing deployment by calling the `client.deployments.delete(deployment_id)` command.\n",
    "To list the existing web services, use `client.deployments.list()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary_and_next_steps\"></a>\n",
    "# Summary and next steps\n",
    "You've successfully completed this notebook!\n",
    "You've learned how to use AutoAI pipeline definition to train the model.\n",
    "Check out the official [AutoAI site](https://www.ibm.com/cloud/watson-studio/autoai) for more samples, tutorials, documentation, how-tos, and blog posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"copyrights\"></a>\n",
    "### Copyrights\n",
    "\n",
    "Licensed Materials - Copyright Â© 2025 IBM. This notebook and its source code are released under the terms of the ILAN License. Use, duplication disclosure restricted by GSA ADP Schedule Contract with IBM Corp.\n",
    "\n",
    "**Note:** The auto-generated notebooks are subject to the International License Agreement for Non-Warranted Programs (or equivalent) and License Information document for Watson Studio Auto-generated Notebook (License Terms), such agreements located in the link below. Specifically, the Source Components and Sample Materials clause included in the License Information document for Watson Studio Auto-generated Notebook applies to the auto-generated notebooks.  \n",
    "\n",
    "By downloading, copying, accessing, or otherwise using the materials, you agree to the <a href=\"https://www14.software.ibm.com/cgi-bin/weblap/lap.pl?li_formnum=L-AMCU-BYC7LF\">License Terms</a>\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
